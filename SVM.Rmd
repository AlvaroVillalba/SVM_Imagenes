---
title: "SVM - Supper Vector Machine"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(survey)
library(sae)
library(TeachingSampling)
library(dplyr)
library(kableExtra)
library(knitr)
library(GGally)
library(survey)
library(TeachingSampling)
library(dplyr)
library(readxl)
library(rgeos)
library(sp)
library(gstat)
library(dplyr)
library(ggplot2)
library(devtools)
library(ggwithimages)
library(leaflet)
library(e1071) 
library(R.utils); library(dplyr)
source("fun.R")
options(scipen = 999)
```

##### Realizado por:

- **Nestor Jardany Serrano Rojas**, [GitHub](https://github.com/jardanys)

<br/>

## OBTENER MNIST (3 & 8)

Generar a bases de datos de **train** y **test** para el número **3** y el número **8**.

```{r data, echo=TRUE}
train_3 <- readRDS("./RDS/Train_3.rds")
train_8 <- readRDS("./RDS/Train_8.rds")

test_3 <- readRDS("./RDS/Test_3.rds")
test_8 <- readRDS("./RDS/Test_8.rds")

train_3_8 <- rbind(train_3, train_8)
test_3_8 <- rbind(test_3, test_8)
```

## PCA

Se observan las líneas con los componentes que absorven el 50%, 80%, 90%, 95%, 98% de la varianza.

```{r pca, echo=TRUE}
pc <- prcomp(train_3_8[,c(1:784)])
pc_test <- predict(pc, test_3_8[,c(1:784)])
plot(cumsum(pc$sdev^2)/sum(pc$sdev^2), xlab = "Componentes", ylab = "Varianza Acumulada", main = "% Acum Varianza")
abline(h=0.5, col="red")
abline(h=0.8, col="blue")
abline(h=0.9, col="green")
abline(h=0.95, col="yellow")
abline(h=0.99, col="pink")
```

## BASES PCA TRAIN & TEST

Se generan las bases de datos a partir del PCA de la base de train.

- **X** base con fixture de train
- **X_test** base con fixture de test
- **Y** base con vector dependiente de train 
- **Y_test** base con vector dependiente de test

```{r train&test, echo=TRUE}
X <- pc$x
X_test <- pc_test
y <- as.numeric(as.character(train_3_8[,785]))
y_test <- as.numeric(as.character(test_3_8[,785]))
```

## SVM

Se genera una base de datos con los resultados de todas las iteraciones teniendo en cuenta los siguientes parámetros:

- **Componentes**, para 2, 10, 40, 60, 80, 100.
- **Kernel**, para todas las posibilidades ("linear", "polynomial", "radial", "sigmoid")
- **Cost**, para 0.1, 1, 10, 100, 1000.

```{r svm, echo=TRUE}
resultados <- data.frame(Componentes=0, Kernel=0, C=0, err_train=0, err_test=0)
Componentes <- c(2, 10, 40, 60, 80, 100)
kernel <- c("linear", "polynomial", "radial", "sigmoid")
C <- c(0.1, 1, 10, 100, 1000)
for(k in Componentes){
      for(i in kernel){
            for(j in C){
                  model_svm <- svm(X[,1:k], as.factor(Y), kernel=i, cost=C)
                  predictivas_train <- predict(model_svm, X[,1:k])
                  predictivas_test <- predict(model_svm, X_test[,1:k])
                  residuales_train <- ifelse(y == predictivas_train,0,1)
                  residuales_test <- ifelse(y_test == predictivas_test,0,1)
                  err_train <- mean(residuales_train)
                  err_test <- mean(residuales_test) 
                  resultados <- rbind(resultados, c(k, i, j, err_train, err_test))
            }
      }
}

resultados <- resultados[-1,]

kable(resultados, "html") %>%
  kable_styling("striped", full_width = F, position = "center") %>%
  scroll_box(width = "850px", height = "300px")
```

## RESULTADOS

```{r resultados, echo=TRUE}
X <- pc$x
X_test <- pc_test
y <- as.numeric(as.character(train_3_8[,785]))
y_test <- as.numeric(as.character(test_3_8[,785]))
```
